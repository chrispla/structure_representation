{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Primary feature choices for cover song identification in da-Tacos\n",
    "Using chroma, crema, hpcp, and a fusion of all, beatsynchronized"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Library importing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dill\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import deepdish as dd"
   ]
  },
  {
   "source": [
    "### Dill session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('datacos_reps.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('datacos_reps.db')"
   ]
  },
  {
   "source": [
    "### Load metadata of subset of performances to work with"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base directory\n",
    "base = '/Volumes/Extreme SSD/da-tacos/'\n",
    "\n",
    "#read metadata\n",
    "with open(base+'da-tacos_metadata/da-tacos_benchmark_subset_metadata.json') as f:\n",
    "    benchmark_metadata = json.load(f)\n",
    "\n",
    "#dictionary holding works:performances:metadata\n",
    "subset_metadata = {}\n",
    "\n",
    "W_count = 0\n",
    "#traverse works\n",
    "for W in benchmark_metadata.keys():\n",
    "    #if it contains at least 5 covers\n",
    "    if len(benchmark_metadata[W].keys()) >= 5: \n",
    "        W_count += 1\n",
    "        per = {}\n",
    "        #get performances\n",
    "        P_count = 0\n",
    "        for P in benchmark_metadata[W].keys():\n",
    "            P_count += 1\n",
    "            per[P] = benchmark_metadata[W][P]\n",
    "            if P_count >= 5: #number of performances per work\n",
    "                break\n",
    "        subset_metadata[W] = per\n",
    "    if W_count >= 50: #number of works\n",
    "        break"
   ]
  },
  {
   "source": [
    "### Compute sets of approximations using chroma, crema, hpcp"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ParameterError",
     "evalue": "width=3 must be at least 1 and at most data.shape[-1]=1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-740a214b8cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#Weighted Recurrence Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mknn_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mssm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'affinity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#Timelag Filter & Path Enchancement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/librosa/segment.py\u001b[0m in \u001b[0;36mrecurrence_matrix\u001b[0;34m(data, k, width, metric, sym, sparse, mode, bandwidth, self, axis)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         raise ParameterError(\n\u001b[0m\u001b[1;32m    419\u001b[0m             \"width={} must be at least 1 and at most data.shape[{}]={}\".format(\n\u001b[1;32m    420\u001b[0m                 \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParameterError\u001b[0m: width=3 must be at least 1 and at most data.shape[-1]=1"
     ]
    }
   ],
   "source": [
    "X = {}\n",
    "\n",
    "#traverse Works\n",
    "for W in subset_metadata.keys():\n",
    "\n",
    "    per = {}\n",
    "    #traverse Performances\n",
    "    for P in subset_metadata[W].keys():\n",
    "\n",
    "        #open performance data\n",
    "        filepath = base + \"da-tacos_benchmark_subset_single_files/\" + W + \"/\" + P + \".h5\"\n",
    "        data = dd.io.load(filepath)\n",
    "\n",
    "        #Beat synchronize MFCCs\n",
    "        synced_mfcc = librosa.util.sync(data['mfcc_htk'], data[\"madmom_features\"][\"onsets\"], aggregate=np.median)\n",
    "\n",
    "        #Similarity Sequence Matrix using Gaussian Kernel\n",
    "        path_distance = np.sum(np.diff(synced_mfcc, axis=1)**2, axis=0)\n",
    "        sigma = np.median(path_distance)\n",
    "        path_sim = np.exp(-path_distance / sigma)\n",
    "        Sloc = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "\n",
    "        rep_set = {}\n",
    "        #features to use\n",
    "        for rep in [\"chroma_cens\", \"crema\", \"hpcp\"]:\n",
    "            \n",
    "            #Beat synchronization\n",
    "            synced = librosa.util.sync(data[rep], data[\"madmom_features\"][\"onsets\"], aggregate=np.median)\n",
    "\n",
    "            #Short-term History Embedding\n",
    "            steps = 4\n",
    "            stacked = librosa.feature.stack_memory(synced, steps)\n",
    "\n",
    "            #Weighted Recurrence Matrix\n",
    "            knn_no = 3\n",
    "            ssm = librosa.segment.recurrence_matrix(stacked, width=knn_no, mode='affinity', sym=True)\n",
    "\n",
    "            #Timelag Filter & Path Enchancement\n",
    "            df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "            Srep = librosa.segment.path_enhance(df(ssm, size=(1,7)), 15)\n",
    "\n",
    "            #Balanced Combination\n",
    "            deg_loc = np.sum(Sloc, axis=1)          \n",
    "            deg_rep = np.sum(Srep, axis=1)\n",
    "            mu = deg_loc.dot(deg_loc + deg_rep) / np.sum((deg_loc + deg_rep)**2)\n",
    "            A = mu * Srep + (1 - mu) * Sloc\n",
    "\n",
    "            #Downsampling\n",
    "            A_d = cv2.resize(A, (256, 256))\n",
    "\n",
    "            #Laplacian\n",
    "            L = scipy.sparse.csgraph.laplacian(A_d, normed=True)\n",
    "\n",
    "            #Eigendecomposition\n",
    "            evals, evecs = scipy.linalg.eigh(L)\n",
    "\n",
    "            #Eigenvector filtering\n",
    "            evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "\n",
    "            #Normalization\n",
    "            Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "\n",
    "            #Set of eigenvector distances\n",
    "            dist_set = []\n",
    "            for k in range(2, 10):\n",
    "                X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "                distance = squareform(pdist(X, metric='euclidean'))\n",
    "                dist_set.append(distance)\n",
    "                \n",
    "            #Add representation\n",
    "            rep_set[rep] = dist_set\n",
    "\n",
    "        #Add representations to performance\n",
    "        per[P] = rep_set\n",
    "\n",
    "    #Add performances to work\n",
    "    X[W] = per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}