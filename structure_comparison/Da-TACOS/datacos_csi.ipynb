{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for Da-Tacos cover song identification from Feature Fused Matrices\n",
    "\n",
    "#Importing\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.interpolate import interp2d\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.linalg import eigh\n",
    "from scipy.ndimage import median_filter\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import cv2\n",
    "from sklearn import metrics\n",
    "import dill\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import deepdish as dd\n",
    "\n",
    "#change matplotlib backend to save rendered plots correctly on linux \n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# #--supress warnings--#\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading 5 works.\n",
      "Loaded Da-TACOS SMMs.\n",
      "Data shape: (50, 4, 128, 128)\n",
      "Formatted 50/50 approximation sets.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#---Load metadata---#\n",
    "with open('/home/ismir/Documents/ISMIR/Datasets/da-tacos/da-tacos_benchmark_subset_metadata.json') as f:\n",
    "    benchmark_metadata = json.load(f)\n",
    "\n",
    "#---Segmentation parameters---#\n",
    "rs_size = 128\n",
    "kmin = 8\n",
    "kmax = 12\n",
    "\n",
    "#---Counters---#\n",
    "count = 0\n",
    "W_count=0\n",
    "P_count = 0\n",
    "\n",
    "#---Loading limits---#\n",
    "min_covers = 10 #load works for which there are at least min_covers performances\n",
    "max_covers = 10 #stop loading performances if over max_covers per work\n",
    "max_works = 5\n",
    "\n",
    "#---Storage---#\n",
    "all_sets = []\n",
    "#all_shapeDNAs = []\n",
    "all_WP = []\n",
    "y = []\n",
    "\n",
    "#for all Works\n",
    "for W in benchmark_metadata.keys():\n",
    "    if len(benchmark_metadata[W].keys()) >= min_covers: #if it contains at least 5 covers\n",
    "        P_count = 0\n",
    "        #for all performances\n",
    "        for P in benchmark_metadata[W].keys():\n",
    "            P_count += 1\n",
    "            \n",
    "            #Computations\n",
    "            try:\n",
    "                SSM = dd.io.load(\"/home/ismir/Documents/ISMIR/Datasets/da-tacosSSMs/StructureLaplacian_datacos_crema_\" + P + \".h5\")['WFused']\n",
    "            except:\n",
    "                print(\"Couldn't load \" + P + \".\")\n",
    "                continue\n",
    "\n",
    "            N = dd.io.load(\"/home/ismir/Documents/ISMIR/Datasets/da-tacosSSMs/StructureLaplacian_datacos_crema_\" + P + \".h5\")['N']\n",
    "\n",
    "            #Construct square matrix from flattened upper triangle\n",
    "            A = np.zeros((N,N))\n",
    "            iN = np.triu_indices(N) #return indices for upper-triangle of (N,N) matrix\n",
    "            for i in range(len(SSM)):\n",
    "                A[iN[0][i]][iN[1][i]] = SSM[i]\n",
    "            B = np.transpose(A)\n",
    "            square_SSM = A+B\n",
    "\n",
    "            #Resample\n",
    "            SSM_ds = cv2.resize(square_SSM, (rs_size,rs_size))\n",
    "\n",
    "            #Compute the Laplacian\n",
    "            L = laplacian(SSM_ds, normed=True)\n",
    "\n",
    "            #Laplacian eigenvalues and eigenvectors\n",
    "            evals, evecs = eigh(L)\n",
    "\n",
    "            # #Shape DNA\n",
    "            # shapeDNA = evals[:30]\n",
    "            # all_shapeDNAs.append(shapeDNA)\n",
    "\n",
    "            #Hierarchical structure\n",
    "            evecs = median_filter(evecs, size=(9, 1))\n",
    "            Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "            # #temporary replacement for bug\n",
    "            # a_min_value = 3.6934424e-08\n",
    "            # Cnorm[Cnorm == 0.0] = a_min_value\n",
    "            # if (np.isnan(np.sum(Cnorm))):\n",
    "            #     print(\"WOOOOOAH\")\n",
    "            \n",
    "            dist_set = []\n",
    "            for k in range(kmin, kmax):\n",
    "                X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "                distance = squareform(pdist(X, metric='euclidean'))\n",
    "                dist_set.append(distance)\n",
    "\n",
    "            all_sets.append(dist_set)\n",
    "            y.append(W)\n",
    "\n",
    "            #append W and P\n",
    "            all_WP.append([W, P])\n",
    "\n",
    "            #plt.matshow()\n",
    "            #plt.colorbar()\n",
    "            #plt.show()\n",
    "\n",
    "            if (P_count >=max_covers):\n",
    "                break\n",
    "                \n",
    "        W_count +=1\n",
    "        sys.stdout.write(\"\\rLoading %i works.\" % W_count)\n",
    "        sys.stdout.flush()\n",
    "        if (W_count >= max_works):\n",
    "            break\n",
    "            \n",
    "all_sets = np.asarray(all_sets)\n",
    "file_no = len(all_WP)\n",
    "# all_shapeDNAs = np.asarray(all_shapeDNAs)\n",
    "\n",
    "print(\"\\nLoaded Da-TACOS SMMs.\")\n",
    "print(\"Data shape:\", all_sets.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[8][i])\n",
    "plt.savefig('/home/ismir/Documents/ISMIR/figures/datacos/approx.png')\n",
    "\n",
    "#------------#\n",
    "#-Formatting-#\n",
    "#------------#\n",
    "\n",
    "all_flat = [] #kmin-kmin sets each with a flattened matrix\n",
    "all_merged = [] #single concatenated vector with all flattened matrices\n",
    "all_shingled2 = [] #shingle adjacent pairs of flat approoximations\n",
    "all_shingled3 = [] #shingle adjacent triples of flat approoximations\n",
    "\n",
    "#traverse songs\n",
    "for f in range(file_no):\n",
    "\n",
    "    #formatting\n",
    "    flat_approximations = []\n",
    "    merged_approximations = np.empty((0))\n",
    "    for j in range(kmax-kmin):\n",
    "        flat_approximations.append(all_sets[f][j].flatten())\n",
    "        merged_approximations = np.concatenate((merged_approximations, flat_approximations[j]))\n",
    "    all_flat.append(np.asarray(flat_approximations))\n",
    "    all_merged.append(merged_approximations)\n",
    "\n",
    "    #shingling per 2\n",
    "    shingled = []\n",
    "    for j in range(kmax-kmin-1):\n",
    "        #shingled.append(np.array([all_flat[f][j],all_flat[f][j+1]]))\n",
    "        shingled.append(np.concatenate((all_flat[f][j],all_flat[f][j+1]), axis=None))\n",
    "    all_shingled2.append(np.asarray(shingled))\n",
    "\n",
    "    #shingling per 3\n",
    "    shingled = []\n",
    "    for j in range(kmax-kmin-2):\n",
    "        #shingled.append(np.array([all_flat[f][j],all_flat[f][j+1],all_flat[f][j+2]]))\n",
    "        shingled.append(np.concatenate((all_flat[f][j],all_flat[f][j+1],all_flat[f][j+2]), axis=None))\n",
    "    all_shingled3.append(np.asarray(shingled))\n",
    "\n",
    "    #progress\n",
    "    sys.stdout.write(\"\\rFormatted %i/%s approximation sets.\" % ((f+1), str(file_no)))\n",
    "    sys.stdout.flush()\n",
    "print('')\n",
    "\n",
    "all_flat = np.asarray(all_flat)\n",
    "all_merged = np.asarray(all_merged)\n",
    "all_shingled2 = np.asarray(all_shingled2)\n",
    "all_shingled3 = np.asarray(all_shingled3)\n",
    "\n",
    "#----------------------#\n",
    "#-Covers vs Non-covers-#\n",
    "#----------------------#\n",
    "\n",
    "#True if cover, False if non-cover\n",
    "covers = np.zeros((len(all_WP), len(all_WP)), dtype=np.bool_)\n",
    "for i in range(len(all_WP)):\n",
    "    for j in range(len(all_WP)):\n",
    "        if (all_WP[i][0] == all_WP[j][0]):\n",
    "            covers[i][j] = True\n",
    "        else:\n",
    "            covers[i][j] = False\n",
    "\n",
    "#-----------#\n",
    "#-Distances-#\n",
    "#-----------#\n",
    "\n",
    "fig_dir = '/home/ismir/Documents/ISMIR/figures/datacos/'\n",
    "\n",
    "#---L1---#\n",
    "L1_distances = np.zeros((file_no, file_no))\n",
    "for i in range(file_no):\n",
    "    for j in range(file_no):\n",
    "        L1_distances[i][j] = np.linalg.norm(all_merged[i]-all_merged[j], ord=1)\n",
    "\n",
    "#Histogram\n",
    "L1_distances_covers = []\n",
    "L1_distances_noncovers = []\n",
    "for i in range(file_no):\n",
    "    for j in range(file_no):\n",
    "        if covers[i][j]:\n",
    "            if (L1_distances[i][j] != 0):\n",
    "                L1_distances_covers.append(L1_distances[i][j])\n",
    "        else:\n",
    "            L1_distances_noncovers.append(L1_distances[i][j])\n",
    "plt.figure()\n",
    "plt.hist(L1_distances_covers, bins=200, alpha=0.5, label='Covers', density=1)\n",
    "plt.hist(L1_distances_noncovers, bins=200, alpha=0.5, label='Non-covers', density=1)\n",
    "plt.title(\"Histogram of L1 distances between cover and non-cover pairs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(fig_dir+'Histogram-L1norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrs = [] #list of cover indeces for that work\n",
    "for cover_idx in range(file_no):\n",
    "    if covers[0][cover_idx] and i!=cover_idx: #if cover and not the same work\n",
    "        cvrs.append(cover_idx)\n",
    "    d = L1_distances[0]\n",
    "    d = np.argsort(d)\n",
    "#     hits = []\n",
    "#     for c in range(len(cvrs)): #traverse covers\n",
    "#         hits.append(np.where(d==c)[0][0])\n",
    "#     hit_positions.append(min(hits))\n",
    "# L1_average_hit = np.mean(hit_positions)\n",
    "# print('L1 mean position of first hit:', L1_average_hit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[0][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[cvrs[0]][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[cvrs[1]][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[d[0]][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, kmax-kmin, figsize=(20, 20))\n",
    "for i in range(kmax-kmin):\n",
    "    axs[i].matshow(all_sets[d[1]][i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}