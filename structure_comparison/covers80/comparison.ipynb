{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Cover Song Identification using comparison of hierarchical structure for covers80"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Library Importing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Dill session"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('sets.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('sets.db')"
   ]
  },
  {
   "source": [
    "### Load audio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose directory containing audiofiles\n",
    "directory = '../../../Datasets/covers80'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all paths in specified directory\n",
    "all_dirs = []\n",
    "all_names= []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "        for name in files:\n",
    "            if (('.wav' in name) or ('.aif' in name) or ('.mp3' in name)):\n",
    "                filepath = os.path.join(root, name)\n",
    "                all_dirs.append(filepath)\n",
    "                all_names.append(name[:-4])\n",
    "file_no = len(all_names)\n",
    "#Load all audiofiles and store in array\n",
    "all_audio = []\n",
    "for f in range(file_no):\n",
    "    y, sr = librosa.load(all_dirs[f], sr=22050, mono=True)\n",
    "    all_audio.append((y, sr))\n",
    "    sys.stdout.write(\"\\rLoaded %i/%s pieces.\" % ((f+1), str(file_no)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "source": [
    "### Hierarchical structure decomposition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CQT config\n",
    "BINS_PER_OCTAVE = 12 * 3\n",
    "N_OCTAVES = 7\n",
    "\n",
    "#Approximations\n",
    "kmin = 2\n",
    "kmax = 8\n",
    "\n",
    "all_struct = []\n",
    "\n",
    "for i in range(file_no):\n",
    "    y, sr = all_audio[i]\n",
    "   \n",
    "    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr,\n",
    "                                        bins_per_octave=BINS_PER_OCTAVE,\n",
    "                                        n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n",
    "                            ref=np.max)\n",
    "    \n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    all_beat_times.append(librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0, x_max=C.shape[1]), sr=sr))\n",
    "\n",
    "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
    "\n",
    "    Cstack = librosa.feature.stack_memory(Csync, 4)\n",
    "    \n",
    "    R = librosa.segment.recurrence_matrix(Cstack, width=3, mode='affinity', sym=True)\n",
    "\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 7))\n",
    "    Rf = librosa.segment.path_enhance(Rf, 15)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    Msync = librosa.util.sync(mfcc, beats)\n",
    "\n",
    "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
    "    sigma = np.median(path_distance)\n",
    "    path_sim = np.exp(-path_distance / sigma)\n",
    "\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "    \n",
    "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
    "    \n",
    "    #eigendecomposition\n",
    "    evals, evecs = scipy.linalg.eigh(L)\n",
    "    #eigenvector filtering\n",
    "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "    #normalization\n",
    "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "    dist_set = []\n",
    "    for k in range(kmin, kmax):\n",
    "        Xs = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "        distance = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(Xs, metric='euclidean'))\n",
    "        dist_set.append(distance)\n",
    "    all_struct.append(dist_set)    \n",
    "\n",
    "    sys.stdout.write(\"\\rComputed for %i/%s pieces.\" % ((f+1), str(file_no)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "source": [
    "### Flatten upper triangle of each approximation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}