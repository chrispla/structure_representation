{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import cv2\n",
    "from seaborn import clustermap\n",
    "import sklearn\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import deepdish as dd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute downsampled Laplacian from Work and Performance name.\n",
    "From http://librosa.github.io/librosa_gallery/auto_examples/plot_segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 25784)\n",
      "(13, 25742)\n"
     ]
    }
   ],
   "source": [
    "def compute_resampled_laplacian(W, P):\n",
    "    \n",
    "    #compose the file path from W, P\n",
    "    crema_path = \"./da-tacos_benchmark_subset_crema/\" + W + \"_crema/\" + P + \"_crema.h5\"\n",
    "    mfcc_path = \"./da-tacos_benchmark_subset_mfcc/\" + W + \"_mfcc/\" + P + \"_mfcc.h5\"\n",
    "    \n",
    "    #load h5 file\n",
    "    crema = np.transpose(dd.io.load(crema_path)[\"crema\"])\n",
    "    mfcc = dd.io.load(mfcc_path)[\"mfcc_htk\"]\n",
    "    \n",
    "    print(crema.shape)\n",
    "    print(mfcc.shape)\n",
    "    \n",
    "    #------------------------------------------------------------------------------# \n",
    "    \n",
    "    #Normally, when we had audio files, we would compute beat event locations to sync with cqt and mfccs\n",
    "    #We don't have any beat data available for datacos (and can't compute them because we don't have the audio time series)\n",
    "    \n",
    "        #Beat tracking\n",
    "        #tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    \n",
    "        #Beat-synchronize crema\n",
    "        #Csync = librosa.util.sync(crema, beats, aggregate=np.median)\n",
    "        \n",
    "        #Beat-synchronize mfcc\n",
    "        #Msync = librosa.util.sync(mfcc, beats)\n",
    "        \n",
    "        \n",
    "    #Do I do some other form of dimensionality reduction (e.g. with 2D interpolation) and proceed as normal?\n",
    "    #Do I not do any dimensionality reduction at this stage, and only care about downsampling the Laplacian at the end?\n",
    "    #If the second question is true, I am confused as to why the shapes of crema and mfccs are always slightly different\n",
    "    #e.g. crema is (12, 25784) while mfcc is (13, 25742) -of course I'm reffering to the second number\n",
    "    #and what do I do to be able to make the later calculations (like for 'mu') that require same size properly \n",
    "        \n",
    "    #------------------------------------------------------------------------------#    \n",
    "    \n",
    "    #Short-term history embedding\n",
    "    Cstack = librosa.feature.stack_memory(crema, 4)\n",
    "\n",
    "    #Building weighted recurrence matrix\n",
    "    R = librosa.segment.recurrence_matrix(Cstack, width=3, mode='affinity', sym=True)\n",
    "    \n",
    "    #Enchancing diagonals with median filter\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 7))\n",
    "    \n",
    "    #Multiangle path enchancement\n",
    "    Rf = librosa.segment.path_enhance(Rf, 15)\n",
    "    \n",
    "    #Building sequence matrix using mfcc-similarity\n",
    "    path_distance = np.sum(np.diff(mfcc, axis=1)**2, axis=0)\n",
    "    sigma = np.median(path_distance)\n",
    "    path_sim = np.exp(-path_distance/sigma)\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "    \n",
    "    #Compute balanced combination\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "    print(path_sim.shape)\n",
    "    print(R_path.shape)\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "    \n",
    "    #Plotting\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    librosa.display.specshow(Rf, cmap='inferno_r', y_axis='time', y_coords=beat_times)\n",
    "    plt.title('Recurrence similarity')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    librosa.display.specshow(R_path, cmap='inferno_r')\n",
    "    plt.title('Path similarity')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    librosa.display.specshow(A, cmap='inferno_r')\n",
    "    plt.title('Combined graph')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "compute_resampled_laplacian(\"W_18\", \"P_83070\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Shape DNA from Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalues(L):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute set of multiple spectral clustering components of Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_set(L):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary with Shape DNAs and sets of multiple spectral clustering components of Laplacian from works with at least one performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./da-tacos_metadata/da-tacos_benchmark_subset_metadata.json') as f:\n",
    "    benchmark_metadata = json.load(f)\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for W in benchmark_metadata.keys():\n",
    "    if len(benchmark_metadata[W].keys()) > 1:\n",
    "        W_dict = {}\n",
    "        for P in benchmark_metadata[W].keys():\n",
    "            #Computations\n",
    "            L_rs = compute_resampled_laplacian(W, P)\n",
    "            shapeDNA = compute_eigenvalues(L_rs)\n",
    "            L_set = compute_clustering_set(L_rs)\n",
    "            \n",
    "            #W Dictionary entry for P\n",
    "            W_dict[P] = [shapeDNA, L_set]\n",
    "        dict[W] = W_dict\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
